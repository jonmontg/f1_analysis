---
title: "Formula 1 Data Analysis"
author: "Jon Montgomery"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

# Formula 1 Data Analysis

### 1.
In this project, we will explore the Formula 1 World Championship data from
1950 to 2024. The data set is available [on kaggle](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020). 
It consists of various CSV files containing tabular data regarding drivers, 
circuits, teams, and results ranging from the granularity of championship
standings to pit stop characteristics. The data are very high quality.

I will be working with Abhishek on this project.

### 2.
We want to use the historical data to create a Markov Chain model to predict
the performance of a driver in the next race. We want to test the effectiveness
of the Markov Chain predictions against naive models to see if we can gain an
edge. Here are some examples of naive models:\newline
1. A driver's performance is random. \newline
2. A driver's performance in the next race will be the same as their performance
in the prior race. \newline
3. A driver's performance in the next race will be equal to the average of their
performance in the prior 10 races. \newline

We will split the data into random samplings of training and test data (60%
training and 40% test). Once the model is trained, we will evaluate the
performance of each model against the test data using the mean absolute error
metric, where the error is the difference between the predicted driver rank
and the actual driver rank.

Once we have calculated the mean absolute error for each model, we will test
the differences in the errors for statistical significance. The method we will
use to test for statistical significance will depend on whether or not the
errors are normally distributed. If the errors are normally distributed, we will
compare the results with the paired t-test. If the errors are not normally
distributed, we will use the Wilcoxon signed-rank test.

### 3.
We will use git for version control in this project. Abhishek and I will both
do preliminary data cleaning and exploration and then merge our findings. I will
build the markov models and most of the code infrastructure for the project.
Then Abhishek will review, we will test the models, and Abhishek will compile
slides for the presentation. Once we have the data, we will write the paper,
dividing it between us depending on who is most familiar with each part.

For cleaning and exploration, we will evaluate each table and field to decide
which variables could hold predictive power and should be included in the
Markov model. We will identify missing values and handle them appropriately
on a case-by-case basis. We will also identify the breakpoints between seasons
with discontinuous car specifications (every 8 years or so the "formula" of the
cars changes drastically). We'll divide the data randomly into testing and
training data and evaluate the models. We will complete this portion of the
project this weekend March 2.

I'll build the Markov Chains between March 3-7 and we will train them and do
analysis over the weekend March 8-9. We will spend the rest of our time preparing
the paper and the presentation.

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
library(ggplot2)
library(knitr)
library(unifed)
library(readr)
library(dplyr)
library(slider)
library(markovchain)
library(xgboost)
library(Matrix)
library(caret)
library(zoo)
library(mltools)

set.seed(53782)
```

Markov Chain Parameters

 - circuit id
 - driver id
 - driver's average rank over the past 10 races
 - driver's most recent rank
 - qualifying position
 - constructor id
 - constructor rank
 - constructor's median pit stop time in the past 10 races
 - weather ?

One row per driver per race.

We will not make predictions for the first race of the season or for drivers with
fewer than 10 races. For each prediction, we only include the information that
we know right before the race starts.

### Read the data into dataframes
```{r}
circuits<-
  read_csv(file.path("data", "circuits.csv"),
           col_types  = cols(
           circuitId  = col_integer(),
           circuitRef = col_character(),
           name       = col_character(),
           location   = col_character(),
           country    = col_character(),
           lat        = col_double(),
           lng        = col_double(),
           alt        = col_integer(),
           url        = col_character())) %>%
  select(-c(
    "circuitRef",
    "location",
    "country",
    "lat",
    "lng",
    "alt",
    "url")) %>%
  rename(circuitName=name)

constructor.standings<-
  read_csv(file.path("data", "constructor_standings.csv"),
           col_types = cols(
             constructorStandingsId = col_integer(),
             raceId                 = col_integer(),
             constructorId          = col_integer(),
             points                 = col_double(),
             position               = col_integer(),
             positionText           = col_character(),
             wins                   = col_integer())) %>%
  select(-c(
    "points",
    "positionText",
    "wins")) %>%
  rename(constructorStandingsPosition = position)

constructors<-
  read_csv(file.path("data", "constructors.csv"),
           col_types = cols(
             constructorId  = col_integer(),
             constructorRef = col_character(),
             name           = col_character(),
             nationality    = col_character(),
             url            = col_character())) %>%
  select(-c(
    "nationality",
    "url",
    "constructorRef")) %>%
  rename(constructorName = name)

driver.standings<-
  read_csv(file.path("data", "driver_standings.csv"),
           col_types = cols(
             driverStandingsId = col_integer(),
             raceId            = col_integer(),
             driverId          = col_integer(),
             points            = col_double(),
             position          = col_integer(),
             positionText      = col_character(),
             wins              = col_integer())) %>%
  select(-c(
    "points",
    "positionText",
    "wins")) %>%
  rename(driverStandingsPosition = position)

drivers<-
  read_csv(file.path("data", "drivers.csv"),
           col_types = cols(
             driverId =    col_integer(),
             driverRef =   col_character(),
             number =      col_character(),
             code =        col_character(),
             forename =    col_character(),
             surname =     col_character(),
             dob =         col_date(),
             nationality = col_character(),
             url =         col_character())) %>%
  select(-c(
    "number",
    "code",
    "dob",
    "nationality",
    "url",
    "driverRef"))

races<-
  read_csv(file.path("data", "races.csv"),
           col_types = cols(
             raceId =    col_integer(),
             year =      col_integer(),
             round =     col_integer(),
             circuitId = col_integer(),
             name =      col_character(),
             date =      col_date(),
             time =      col_character(),
             url =       col_character(),
             fp1_date =  col_character(),
             fp1_time =  col_character())) %>%
  select(-c(
    "year",
    "name",
    "time",
    "url",
    "fp1_date",
    "fp1_time",
    "fp2_date",
    "fp2_time",
    "fp3_date",
    "fp3_time",
    "quali_date",
    "quali_time",
    "sprint_date",
    "sprint_time"))

results<-
  read_csv(file.path("data", "results.csv"),
           col_types = cols(
             resultId =      col_integer(),
             raceId =        col_integer(),
             driverId =      col_integer(),
             constructorId = col_integer(),
             number =        col_character(),
             grid =          col_integer(),
             position =      col_character(),
             positionText =  col_character(),
             positionOrder = col_integer(),
             points =        col_double())) %>%
  select(-c(
    "number",
    "position",
    "positionText",
    "points",
    "laps",
    "time",
    "milliseconds",
    "fastestLap",
    "rank",
    "fastestLapTime",
    "fastestLapSpeed")) %>%
  rename(raceResultPosition = positionOrder, startingPosition = grid)

status<-
  read_csv(file.path("data", "status.csv"),
           col_types = cols(
             statusId = col_integer(),
             status =   col_character()
           ))
```
### Creating the usable data

We only include results for drivers that have at least 10 races.

```{r}
get_mode <- function(x) {
  unique_x <- unique(x)
  unique_x[which.max(tabulate(match(x, unique_x)))]
}

get_qualitative_race_result <- function(x) {
  return(case_when(
      x == 1 ~         "win",
      x %in% c(2, 3) ~ "podium",
      x <= 10 ~        "score",
      TRUE ~           "no score"
    ))
}
encoded_qualitative_race_result <- function(x) {
  return(case_when(
    x == "win" ~      0,
    x == "podium" ~   1,
    x == "score" ~    2,
    x == "no score" ~ 3
  ))
}
decode_qualitative_race_result <- function(x) {
  return(case_when(
    x == 0 ~ "win",
    x == 1 ~ "podium",
    x == 2 ~ "score",
    x == 3 ~ "no score"
  ))
}

# Joining the datasets
data <- results %>%
  left_join(races, by = "raceId") %>%
  left_join(circuits, by = "circuitId") %>%
  left_join(drivers, by = "driverId") %>%
  left_join(constructors, by = "constructorId") %>%
  left_join(driver.standings, by = c("raceId", "driverId")) %>%
  left_join(constructor.standings, by = c("raceId", "constructorId")) %>%
  left_join(status, by = "statusId") %>%
  arrange(driverId, date) %>% # ensure the data is ordered correctly
  group_by(driverId) %>%
  mutate(
    meanRaceResult10 = slider::slide_dbl(raceResultPosition, mean, .before = 9, .complete = TRUE),
    status = case_when(
      status == "Finished" ~         "Finished",
      grepl("\\+\\d+ Lap", status) ~ "Lapped",
      TRUE ~                         "DNF"
    ),
    raceResultQualitative = get_qualitative_race_result(raceResultPosition)
  ) %>%
  mutate(
    modeQualitativeResult10 = rollapply(raceResultQualitative, width = 10, FUN = get_mode, 
                                         align = "right", fill = NA)
  ) %>%
  mutate(
    prevRaceResultPosition =           lag(raceResultPosition),
    prevDriverStandingsPosition =      lag(driverStandingsPosition),
    prevConstructorStandingsPosition = lag(constructorStandingsPosition),
    prevStatus =                       lag(status),
    prevMeanRaceResult10 =             lag(meanRaceResult10),
    prevModeQualitativeResult10 =      lag(modeQualitativeResult10),
    prevRaceResultQualitative =        lag(raceResultQualitative)
  ) %>%
  ungroup() %>%
  filter(!is.na(prevMeanRaceResult10) & !is.na(prevRaceResultPosition) & (round!=1))
data$driverStandingsPosition[is.na(data$driverStandingsPosition)] <- max(data$driverStandingsPosition, na.rm = TRUE)
data$constructorStandingsPosition[is.na(data$constructorStandingsPosition)] <- max(data$constructorStandingsPosition, na.rm = TRUE)
data$prevStatus[is.na(data$prevStatus)] <- "unknown"
```
### Splitting the training and testing data
```{r}
train.index <- createDataPartition(1:nrow(data), p = 0.6, list = FALSE)
train.data <- data[train.index,]
test.data <- data[-train.index,]
```
### Building the Markov Chain
```{r}
get.trained.mc <- function(training.data.from, training.data.to) {
  # Ensure all unique states are included (as character)
  all_states <- as.character(unique(c(training.data.from, training.data.to)))

  # Create a transition table (ensuring all states are included)
  transition.table <- table(
    factor(training.data.from, levels = all_states), 
    factor(training.data.to, levels = all_states)
  )

  # Convert the table to a probability matrix
  transition.matrix <- prop.table(transition.table, margin = 1)

  # Explicitly convert "table" to a numeric matrix
  transition.matrix <- matrix(as.numeric(transition.matrix), 
                              nrow = length(all_states), 
                              dimnames = list(all_states, all_states))

  # Create and return the Markov Chain model
  return(new("markovchain", states = all_states, transitionMatrix = transition.matrix))
}
mc.predict<-function(mc, test.data) {
  return(sapply(test.data, function(x) rmarkovchain(n = 1, object = mc, t0 = x)))
}
```
### XGBoost
```{r}
library(data.table)
library(mltools)

# Convert categorical variables using one-hot encoding
data.xgboost <- as.data.table(data)
data.xgboost[, `:=`(
  circuitId = as.factor(circuitId),
  constructorId = as.factor(constructorId),
  prevStatus = as.factor(prevStatus),
  raceResultQualitative = encoded_qualitative_race_result(raceResultQualitative)
)]

data.onehot <- one_hot(as.data.table(data.xgboost[, c(
  "circuitId",
  "constructorId",
  "prevStatus"
)]))

# Select numerical features
data.numeric <- data.xgboost[, c(
  "startingPosition",
  "prevDriverStandingsPosition",
  "prevConstructorStandingsPosition",
  "prevRaceResultPosition",
  "prevMeanRaceResult10",
  "raceResultQualitative"
)]

# Combine numerical and one-hot encoded categorical features
data.xgboost <- cbind(data.numeric, data.onehot)

train.data.xgboost <- data.xgboost[train.index, ]
test.data.xgboost <- data.xgboost[-train.index, ]

# Separate features and labels
train.x <- as.matrix(train.data.xgboost[, !names(train.data.xgboost) %in% "raceResultQualitative", with = FALSE])
train.y <- train.data.xgboost$raceResultQualitative

test.x <- as.matrix(test.data.xgboost[, !names(test.data.xgboost) %in% "raceResultQualitative", with = FALSE])
test.y <- test.data.xgboost$raceResultQualitative

# Convert to XGBoost DMatrix
dtrain <- xgb.DMatrix(data = train.x, label = train.y)
dtest <- xgb.DMatrix(data = test.x, label = test.y)

params <- list(
  objective = "multi:softmax", # Multi-class classification
  num_class = length(unique(data$raceResultQualitative)), # Number of classes
  eval_metric = "mlogloss", # Multi-class log loss
  eta = 0.1, # Learning rate
  max_depth = 6, # Tree depth
  subsample = 0.8, # Sample ratio
  colsample_bytree = 0.8 # Column sample ratio
)

# Train the XGBoost model
model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain, test = dtest),
  verbose = 1
)
preds <- predict(model, dtest)

# Convert numeric predictions back to original categories
xgboost.predictions <- as.factor(preds)
xgboost.actual <- as.factor(test.y)
```

### Naive Models
```{r}
naive.predict.same.as.last<-function(test.data) {
  return(test.data$prevRaceResultQualitative)
}
naive.predict.same.as.mode<-function(test.data) {
  return(test.data$prevModeQualitativeResult10)
}
naive.predict.same.as.start<-function(test.data) {
  return(sapply(test.data$startingPosition, function(x) get_qualitative_race_result(x)))
}
```
### Training the Markov Chain
```{r}
mc <- get.trained.mc(train.data$prevRaceResultQualitative, train.data$raceResultQualitative)
```

### Generate all the predictions
```{r}
comparison.report <- function(preds, actual) {
  preds <- factor(sapply(preds, encoded_qualitative_race_result))
  actual <- factor(sapply(actual, encoded_qualitative_race_result))

  # Ensure both factors have the same levels
  levels_combined <- union(levels(preds), levels(actual))
  preds <- factor(preds, levels = levels_combined)
  actual <- factor(actual, levels = levels_combined)

  return(confusionMatrix(preds, actual))
}
markov.predictions <- mc.predict(mc, test.data$prevRaceResultQualitative)
naive.same.as.last <- naive.predict.same.as.mode(test.data)
naive.same.as.mode <- naive.predict.same.as.mode(test.data)
naive.same.as.start <- naive.predict.same.as.start(test.data)
```
### Statistics for the Markov Model
Let's use the Chi-Squared test to see if the predictions are statistically
correlated to the actual results.

```{r}
print(chisq.test(table(markov.predictions, test.data$raceResultQualitative)))
print(comparison.report(markov.predictions, test.data$raceResultQualitative))
```
### Statistics for the Naive Same as Mode
```{r}
print(chisq.test(table(naive.same.as.mode, test.data$raceResultQualitative)))
print(comparison.report(naive.same.as.mode, test.data$raceResultQualitative))
```
### Statistics for Naive Same as Last
```{r}
print(chisq.test(table(naive.same.as.last, test.data$raceResultQualitative)))
print(comparison.report(naive.same.as.last, test.data$raceResultQualitative))
```
### Statistics for Naive Same as Starting Position
```{r}
print(chisq.test(table(naive.same.as.start, test.data$raceResultQualitative)))
print(comparison.report(naive.same.as.start, test.data$raceResultQualitative))
```
### Statistics for the XGBoost Model
```{r}
print(chisq.test(table(xgboost.predictions, test.data$raceResultQualitative)))
print(confusionMatrix(xgboost.predictions, xgboost.actual))
```
### Interpreting the Confusion Matrix
A confusion matrix is a tool for evaluating the performance of a predictive model
used when you have labeled categorical data. It shows the predicted category on
one axis and the true category on the other. Each cell contains the number of
observations for that cross section.

The Accuracy metric measures the total number of correct classifications
divided by the total number of exambles:

$$
Accuracy(a, p) = \frac{1}{N}\sum_{i=1}^{N}
\begin{cases}
1 & \text {if }a_i=p_i \\
0 & \text{else}
\end{cases}
$$
The confidence interval for an accuracy score is calculated via Wilson's score
interval. The Wilson score interval is asymmetric, and it doesn't suffer from problems of overshoot and zero-width intervals. It can be safely employed with
small samples and skewed observations. 

$$
z_{a} \approx \frac{(p-\hat{p})}{\sigma_{n}}
$$
where $z_{a}$ is the standard normal deviation for the desired confidence interval
$1-a$, $p$ is the 'true' accuracy, $hat{p}$ is the observed accuracy, and $\sigma_{n}$
is the binomial sample standard deviation. The derivation of the Wilson score
interval is beyond the scope of this paper.

### Comparing the accuracy of our predictions
All of our models showed significant correlation between the predictions
and outcomes, but which model performed best? \\

Markov Model
  Accuracy : 0.4381         
  95% CI : (0.4272, 0.449)
  
Same as Last
  Accuracy : 0.5186          
  95% CI : (0.5077, 0.5296)
  
Same as Starting Position
  Accuracy : 0.5371         
  95% CI : (0.5262, 0.548)
  
Same as Mode
  Accuracy : 0.5672          
  95% CI : (0.5564, 0.5781)
  
XGBoost
  Accuracy : 0.604           
  95% CI : (0.5933, 0.6147)
  
  
We will use Paired McNemar's Test to compare these accuracy scores because
each model made predictions from the same dataset. Our null hypothesis
is that there is no difference in accuracy between any of the models.

McNemar's Test is the following:
\begin{table}
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
                        & Model B Correct & Model B Incorrect \\
      Model A Correct   & $n_{11}$        & $n_{10}$          \\
      Model A Incorrect & $n_{01}$        & $n_{00}$          \\
    \hline
  \end{tabular}
\end{table}

McNemar's test statistic is:
$$
\chi^{2}=\frac{(n_{10}-n_{01})^2}{n_{10}+n_{01}}
$$
Where $\chi^{2}$ has a chi-squared distribution with 1 degree of freedom.

We will perform McNemar's test on each pair of predictions from least
to most accurate. If we have predictions $a, b, c$ with $accuracy(a)>accuracy(b)$ and $accuracy(b)>accuracy(c)$, we can say that $accuracy(a)>accuracy(b)>accuracy(c)$.

### Markov to Same as Last
```{r}
mcnemar.compare<-function(predictions.a, predictions.b, actual) {
  correct.a <- as.integer(predictions.a == actual)
  correct.b <- as.integer(predictions.b == actual)
  a <- sum(correct.a == 1 & correct.b == 1)
  b <- sum(correct.a == 1 & correct.b == 0)
  c <- sum(correct.a == 0 & correct.b == 1)
  d <- sum(correct.a == 0 & correct.b == 0)
  return(
    mcnemar.test(matrix(
      c(a, b, c, d), nrow=2, byrow=TRUE,
      dimnames=list("Model 1" = c("Correct", "Incorrect"),
                    "Model 2" = c("Correct", "Incorrect"))
  ))$p.value)
}
print(paste("Markov v Same as Last: p =",
            mcnemar.compare(markov.predictions, naive.same.as.last, test.data$raceResultQualitative)))
print(paste("Same as Last v Same as Start: p =",
            mcnemar.compare(naive.same.as.last, naive.same.as.start, test.data$raceResultQualitative)))
print(paste("Same as Start v Same as Mode: p =",
            mcnemar.compare(naive.same.as.start, naive.same.as.mode, test.data$raceResultQualitative)))
print(paste("Same as Mode v XGBoost: p =",
            mcnemar.compare(naive.same.as.mode, sapply(xgboost.predictions, decode_qualitative_race_result), test.data$raceResultQualitative)))
```

### Given a model, how does its accuracy, compare across each category?
```{r}
plot.category.accuracy<-function(name, pred, actual) {
  adf <- data.frame(actual, pred) %>%
    mutate(correct = actual == pred) %>%
    group_by(actual) %>%
    summarise(accuracy = mean(correct))
  return(ggplot(adf, aes(x = actual, y = accuracy, fill = actual)) +
    geom_bar(stat = "identity") +
    labs(title = paste(name, "Prediction Accuracy Per Result"),
         x = "Outcome",
         y = "Accuracy") +
    theme_minimal())
}
markov.predictions.display <- data.frame(prediction = markov.predictions, actual = test.data$raceResultQualitative)
naive.same.as.last.display <- data.frame(prediction = naive.same.as.last, actual = test.data$raceResultQualitative)
naive.same.as.mode.display <- data.frame(prediction = naive.same.as.mode, actual = test.data$raceResultQualitative)
naive.same.as.start.display <- data.frame(prediction = naive.same.as.start, actual = test.data$raceResultQualitative)
xgboost.predictions.display <- data.frame(prediction = sapply(xgboost.predictions, decode_qualitative_race_result), actual = test.data$raceResultQualitative)

markov.predictions.display$model  <- "Markov"
naive.same.as.last.display$model  <- "Same as Last"
naive.same.as.mode.display$model  <- "Same as Mode"
naive.same.as.start.display$model <- "Same as Start"
xgboost.predictions.display$model <- "XGBoost"

display.predictions.data <- bind_rows(
  markov.predictions.display,
  naive.same.as.last.display,
  naive.same.as.mode.display,
  naive.same.as.start.display,
  xgboost.predictions.display
)

adf <- display.predictions.data %>%
  group_by(model, actual) %>%
  mutate(correct = actual == prediction) %>%
  summarise(accuracy = mean(correct))
print(adf)

ggplot(adf, aes(x = actual, y = accuracy, fill = model)) +
  geom_bar(stat = "identity", position = "dodge") +  # Use bar chart instead of histogram
  labs(title = "Model Accuracy Per Category", x = "Actual Category", y = "Accuracy") +
  theme_minimal()

```
